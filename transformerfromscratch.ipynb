{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8BDcotN81vT0VyG51qTdu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahchhatru/AI_colab_notebooks/blob/main/transformerfromscratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FerIZpIAgkEF",
        "outputId": "dc4d4746-68db-4f55-fdc3-b32853c13ac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.0+cu116\n"
          ]
        }
      ],
      "source": [
        "# importing required libraries\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math,copy,re\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torchtext\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.simplefilter(\"ignore\")\n",
        "print(torch.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create word embeddings\n",
        "First of all we need to convert each word to an embedding vector.Embedding vectors create more semantic representation of word.\n",
        "Suppose each embedding vector is of size 512 and our embedding matrix is size of 100*512 .The matrix will be learned on training and during inference each word will be mapped to the corresponding 512d vector. Suppose we have a batch size of 32 and sequence of length 10 the the output will be 32x10x512 matrix."
      ],
      "metadata": {
        "id": "E6pEdDETjeyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding class\n",
        "class Embedding(nn.Module):\n",
        "  def __init__(self,vocab_size,embed_dim):\n",
        "    \"\"\"\n",
        "    arg vocab size :size of vocabulary\n",
        "    embeded_dim:dimension of embeddings.\n",
        "    \"\"\"\n",
        "    super(Embedding,self).__init__()\n",
        "    self.embed=nn.Embedding(vocab_size,embed_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    \"\"\"\n",
        "    args\n",
        "    x :input_vector\n",
        "\n",
        "    returns\n",
        "    out:dimension_of_dim\n",
        "\n",
        "    \"\"\"\n",
        "    out=self.embed(x)\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "OnNpFlxjgtt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# positional encoding\n",
        "Next step is to generate positional encoding . Inorder for the model to make sense of the word it needs to know about the meaning of the sentence and it's position in the sentence.\n",
        "\n",
        "In attention is all you need paper . The author used the following functions to create positional encodings.\n",
        "on odd time steps a cosine function is used and on even time steps a\n",
        "sine function is used.\n",
        "```\n",
        "\\PE_(pos,2i)=sin(pos/10000^(2*i/d))\\\n",
        "\n",
        "\\PE_(pos,2i+1)=cos(pos/10000^(2*i/d))\\\n",
        "\n",
        "```\n",
        "pos -> refers to order in the sentence\n",
        "i -> refers to position along embedding vector dimension\n",
        "\n",
        "positional embedding will generate a matrix similar to embedding matrix.\n",
        "it will create a matrix of dimension sequence_length x embedding_dimension\n",
        "for each word we will get a embedding vector which is of dimension 1x512 and similarly we will also find a corresponding positional vector of 1x512 to get a 1x512 vector for each word.\n",
        "\n",
        "or eg: if we have batch size of 32 and seq length of 10 and let embedding dimension be 512. Then we will have embedding vector of dimension 32 x 10 x 512. Similarly we will have positional encoding vector of dimension 32 x 10 x 512. Then we add both.\n"
      ],
      "metadata": {
        "id": "g4t9gTF4qubj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## register buffer in Pytorch ->\n",
        "# If you have parameters in your model, which should be saved and restored in the state_dict,\n",
        "# but not trained by the optimizer, you should register them as buffers.\n",
        "\n"
      ],
      "metadata": {
        "id": "nq9X8lkdojyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self,max_seq_len,embed_model_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            seq_len: length of input sequence\n",
        "            embed_model_dim: demension of embedding\n",
        "        \"\"\"\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.embed_dim = embed_model_dim\n",
        "\n",
        "        pe = torch.zeros(max_seq_len,self.embed_dim)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0,self.embed_dim,2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/self.embed_dim)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/self.embed_dim)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input vector\n",
        "        Returns:\n",
        "            x: output\n",
        "        \"\"\"\n",
        "\n",
        "        # make embeddings relatively larger\n",
        "        x = x * math.sqrt(self.embed_dim)\n",
        "        #add constant to embedding\n",
        "        seq_len = x.size(1)\n",
        "        x = x + torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False)\n",
        "        return x"
      ],
      "metadata": {
        "id": "b0PooHP2wicE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#self attention and mutihead attention\n",
        "### self attention\n",
        "suppose we have the sentence dog is crossing the street 'Dog is crossing the street because it saw the kitten' here we know it refers to Dog but the machine won't understand it.\n",
        "As model proceeses each word, self attention allows it to look at other positions in the input sequence for clues. It will creates a vector based on dependency of each word with the other.\n",
        "\n",
        "Let us go through a step by step illustration of self attention.\n",
        "\n",
        "Step 1: The first step in calculating self-attention is to create three vectors from each of the encoderâ€™s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. Each of the vector will be of dimension 1x64.\n",
        "Since we have a multihead attention we will have 8 self attention heads.I will explain the code with 8 attention head in mind."
      ],
      "metadata": {
        "id": "6_IAmeKC1wd9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KoqPmL3R1q_H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}