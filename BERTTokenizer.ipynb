{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPw3s3aznzLkb4mgFIITlJ8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahchhatru/AI_colab_notebooks/blob/main/BERTTokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## following youtube tutorial\n",
        "https://www.youtube.com/watch?v=KPtna8FahZ8&list=PLxqBkZuBynVQaqvEwN-qAjkNAJ6NgyfcM\n",
        "\n",
        "Notebook no 1"
      ],
      "metadata": {
        "id": "o2siBGYzfcFU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-raruIpQfVRz"
      },
      "outputs": [],
      "source": [
        "! pip install transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "gU5tbLF7mnYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_s4dO30m0Er",
        "outputId": "f713990a-95b2-40d0-c20e-e23001510cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence='She is a machine learning engineer and works in California'\n"
      ],
      "metadata": {
        "id": "AZAQtAB1p_Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "4a_2nJQaqJt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenize the sentences and obtain the token data[link text](https://)"
      ],
      "metadata": {
        "id": "5twKx84ept3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens=tokenizer.tokenize(sentence)"
      ],
      "metadata": {
        "id": "u_uznnNYptJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q2sEHLLqY2r",
        "outputId": "1167bd5a-c40e-4a2e-d039-f345917bb247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['she',\n",
              " 'is',\n",
              " 'a',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'engineer',\n",
              " 'and',\n",
              " 'works',\n",
              " 'in',\n",
              " 'california']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADD stop and start token to the original token list\n",
        "\n",
        "'[CLS]' for begging and '[SEP]' for ending\n",
        "\n"
      ],
      "metadata": {
        "id": "VSR4hSzWq_it"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens=['[CLS]']+tokens+['[SEP]']"
      ],
      "metadata": {
        "id": "28pMwC7zq-y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0cvjwyTriuU",
        "outputId": "78c14ee6-0aa5-4aa0-e2f2-8b6921bf56b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'she',\n",
              " 'is',\n",
              " 'a',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'engineer',\n",
              " 'and',\n",
              " 'works',\n",
              " 'in',\n",
              " 'california',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we tokenize the sentences we need a list of token of same size as a input to our model. But we know that sentences length are not fixed naturally. They vary . So what do we do??\n",
        "We simply set a max_length limit and add padding to the sentence until it's token array is of that size."
      ],
      "metadata": {
        "id": "npQnpZeZs2d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##padding\n",
        "## here let's us consider max_length is 14 so we need to add two padddings\n",
        "tokens=tokens+['[PAD]']+['[PAD]']"
      ],
      "metadata": {
        "id": "ihzah8I3s0sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJacLZh-tqjh",
        "outputId": "3bba95ae-c1f0-4b28-e393-9fa488cbc2fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'she', 'is', 'a', 'machine', 'learning', 'engineer', 'and', 'works', 'in', 'california', '[SEP]', '[PAD]', '[PAD]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4re6jsy-tuKf",
        "outputId": "410ab600-625a-4b2b-8a29-d0cc63102b16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Great now let's make our model understand that the pad token is added only to match the token length and is not part of the actual token.  for this we create a attention mask where 0 is for PAD tokens and 1 for rest of the tokens\n"
      ],
      "metadata": {
        "id": "n1L-Gwujzlpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_mask=[1 if i!='[PAD]' else 0 for i in tokens]"
      ],
      "metadata": {
        "id": "vgVwj-P3z8xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(attention_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBfSON6J0S12",
        "outputId": "4a7b8fa3-8b13-4a05-e033-6e2b4ca5e893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unique token id\n",
        "It is a concept related to mapping all the tokens to an unique id\n",
        "We can simply do that here with a method called convert token to id\n"
      ],
      "metadata": {
        "id": "AWnxq2WB1Ksn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids=tokenizer.convert_tokens_to_ids(tokens)"
      ],
      "metadata": {
        "id": "WceSp93p0h8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viWyhNBO1ur0",
        "outputId": "f27e0931-174f-4bdd-da25-ea92d442d43f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101, 2016, 2003, 1037, 3698, 4083, 3992, 1998, 2573, 1999, 2662, 102, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids=torch.tensor(token_ids).unsqueeze(0)\n",
        "## similarly for attention masks as well\n",
        "attention_mask=torch.tensor(attention_mask).unsqueeze(0)\n",
        "print(token_ids)\n",
        "print(attention_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THfBXzT65z8S",
        "outputId": "cec0b783-3d2a-4060-85c9-688fb7effd7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 101, 2016, 2003, 1037, 3698, 4083, 3992, 1998, 2573, 1999, 2662,  102,\n",
            "            0,    0]])\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs=model(token_ids,attention_mask=attention_mask)"
      ],
      "metadata": {
        "id": "QptYtAel75i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTvnM53Z8DXg",
        "outputId": "bd9236c8-bbc0-42ba-9315-deae93dc94df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1054,  0.1832, -0.4909,  ..., -0.3241,  0.3984,  0.1591],\n",
              "         [ 0.2438, -0.3654, -0.6240,  ..., -0.0516,  0.3615, -0.3675],\n",
              "         [-0.1027,  0.1559,  0.0640,  ..., -0.5743,  0.0936,  0.2774],\n",
              "         ...,\n",
              "         [ 0.7337,  0.0462, -0.4839,  ..., -0.0072, -0.5861, -0.5907],\n",
              "         [-0.1763,  0.0910, -0.6007,  ...,  0.1243,  0.4635, -0.3595],\n",
              "         [-0.2029, -0.1064, -0.5744,  ...,  0.4472,  0.6531, -0.4184]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.9473, -0.4997, -0.8718,  0.8811,  0.7894, -0.3033,  0.9334,  0.4975,\n",
              "         -0.7602, -1.0000, -0.6657,  0.9340,  0.9871,  0.4571,  0.9601, -0.8242,\n",
              "         -0.1882, -0.7192,  0.4297, -0.7576,  0.7524,  1.0000,  0.2104,  0.4064,\n",
              "          0.5661,  0.9898, -0.8310,  0.9563,  0.9695,  0.8257, -0.7848,  0.4256,\n",
              "         -0.9918, -0.3059, -0.8785, -0.9956,  0.5351, -0.8573, -0.0809, -0.1058,\n",
              "         -0.9060,  0.5226,  1.0000, -0.1399,  0.5261, -0.3048, -1.0000,  0.3643,\n",
              "         -0.9460,  0.8846,  0.7668,  0.8117,  0.2881,  0.6517,  0.5824, -0.2660,\n",
              "          0.0118,  0.1780, -0.3221, -0.7566, -0.6814,  0.4362, -0.8407, -0.9596,\n",
              "          0.8762,  0.7554, -0.2953, -0.3134, -0.2160, -0.0667,  0.9666,  0.3424,\n",
              "          0.0743, -0.8780,  0.6402,  0.2758, -0.7399,  1.0000, -0.5168, -0.9872,\n",
              "          0.7122,  0.7422,  0.7109, -0.1680,  0.4112, -1.0000,  0.6379, -0.1462,\n",
              "         -0.9948,  0.2089,  0.5989, -0.3234,  0.3302,  0.7130, -0.4732, -0.5511,\n",
              "         -0.4602, -0.8095, -0.3819, -0.4802,  0.1824, -0.3377, -0.4189, -0.5157,\n",
              "          0.3147, -0.5996, -0.6470,  0.3816, -0.0048,  0.7138,  0.5924, -0.4112,\n",
              "          0.4949, -0.9682,  0.7296, -0.3471, -0.9895, -0.7341, -0.9937,  0.8239,\n",
              "         -0.3895, -0.3774,  0.9832,  0.2486,  0.5444, -0.2425, -0.9053, -1.0000,\n",
              "         -0.6852, -0.1713, -0.0852, -0.2448, -0.9874, -0.9646,  0.6930,  0.9652,\n",
              "          0.3507,  1.0000, -0.5356,  0.9507, -0.3166, -0.7947,  0.5744, -0.5999,\n",
              "          0.7252,  0.4883, -0.7820,  0.3282, -0.5786,  0.1877, -0.7344, -0.3768,\n",
              "         -0.5342, -0.9593, -0.4488,  0.9617, -0.6580, -0.9190,  0.0132, -0.4360,\n",
              "         -0.5127,  0.8982,  0.7884,  0.4786, -0.4304,  0.5496,  0.2895,  0.6779,\n",
              "         -0.8821, -0.1980,  0.5438, -0.4306, -0.7744, -0.9862, -0.5156,  0.6959,\n",
              "          0.9913,  0.8366,  0.3724,  0.8100, -0.4484,  0.7909, -0.9716,  0.9887,\n",
              "         -0.2762,  0.2494, -0.0682,  0.2337, -0.9039, -0.0255,  0.9031, -0.8000,\n",
              "         -0.9244, -0.1093, -0.5341, -0.5321, -0.7741,  0.6869, -0.4124, -0.5095,\n",
              "         -0.2599,  0.9482,  0.9917,  0.8116,  0.1873,  0.7582, -0.9626, -0.5284,\n",
              "          0.2655,  0.4019,  0.2757,  0.9947, -0.6125, -0.3087, -0.9465, -0.9873,\n",
              "          0.0295, -0.9428, -0.2360, -0.7415,  0.7049, -0.5888,  0.5658,  0.4980,\n",
              "         -0.9931, -0.8473,  0.4439, -0.5102,  0.5792, -0.2948,  0.6668,  0.8968,\n",
              "         -0.7408,  0.6627,  0.9338, -0.7714, -0.8636,  0.8998, -0.3689,  0.9307,\n",
              "         -0.7885,  0.9982,  0.8852,  0.8113, -0.9649, -0.6236, -0.9545, -0.6879,\n",
              "         -0.1642,  0.0901,  0.8598,  0.7472,  0.4841,  0.4896, -0.6265,  0.9993,\n",
              "         -0.8253, -0.9635, -0.3037, -0.3706, -0.9913,  0.8040,  0.4028,  0.3308,\n",
              "         -0.5436, -0.7377, -0.9666,  0.9334,  0.3143,  0.9961, -0.4325, -0.9580,\n",
              "         -0.7158, -0.9488, -0.0229, -0.2287, -0.3045, -0.0570, -0.9769,  0.5914,\n",
              "          0.6860,  0.7058, -0.8151,  0.9995,  1.0000,  0.9809,  0.9387,  0.9618,\n",
              "         -0.9999, -0.6581,  1.0000, -0.9914, -1.0000, -0.9708, -0.7347,  0.4207,\n",
              "         -1.0000, -0.1913, -0.0066, -0.9400,  0.6133,  0.9789,  0.9978, -1.0000,\n",
              "          0.9457,  0.9712, -0.7599,  0.9351, -0.4134,  0.9786,  0.4849,  0.6907,\n",
              "         -0.4154,  0.5477, -0.9038, -0.9119, -0.6370, -0.7353,  0.9977,  0.2083,\n",
              "         -0.8779, -0.9488,  0.6546, -0.0561, -0.2484, -0.9743, -0.3691,  0.5904,\n",
              "          0.7463,  0.3033,  0.4226, -0.8184,  0.3554, -0.2778,  0.6312,  0.7593,\n",
              "         -0.9528, -0.7666, -0.1611, -0.1131, -0.5096, -0.9678,  0.9837, -0.6137,\n",
              "          0.8217,  1.0000,  0.1718, -0.9574,  0.7098,  0.3458,  0.0616,  1.0000,\n",
              "          0.8249, -0.9897, -0.6642,  0.7551, -0.7042, -0.7526,  0.9999, -0.3963,\n",
              "         -0.6082, -0.4559,  0.9839, -0.9915,  0.9928, -0.9337, -0.9776,  0.9842,\n",
              "          0.9614, -0.5498, -0.8179,  0.1782, -0.6581,  0.2918, -0.9793,  0.7885,\n",
              "          0.5491, -0.2452,  0.9236, -0.8597, -0.6369,  0.4439, -0.4519,  0.0215,\n",
              "          0.8996,  0.5861, -0.2864,  0.1808, -0.4304, -0.2213, -0.9835,  0.5373,\n",
              "          1.0000, -0.1097,  0.5654, -0.4920, -0.2247,  0.0190,  0.6075,  0.6431,\n",
              "         -0.3465, -0.9370,  0.6292, -0.9790, -0.9924,  0.8198,  0.2410, -0.3098,\n",
              "          1.0000,  0.5190,  0.3379,  0.3158,  0.9834,  0.0082,  0.6649,  0.8096,\n",
              "          0.9853, -0.3637,  0.6894,  0.9305, -0.8707, -0.4580, -0.7281,  0.0286,\n",
              "         -0.9205,  0.0083, -0.9816,  0.9805,  0.9303,  0.4153,  0.3625,  0.5934,\n",
              "          1.0000, -0.6325,  0.7459, -0.5007,  0.8037, -0.9997, -0.9086, -0.4583,\n",
              "         -0.0590, -0.6986, -0.3440,  0.3194, -0.9842,  0.7109,  0.7074, -0.9904,\n",
              "         -0.9906, -0.1722,  0.9065,  0.2480, -0.9800, -0.7749, -0.6696,  0.7887,\n",
              "         -0.3360, -0.9624, -0.0304, -0.4221,  0.6519, -0.4084,  0.6637,  0.7864,\n",
              "          0.7471, -0.6538, -0.4711, -0.2918, -0.8675,  0.8240, -0.9192, -0.9288,\n",
              "         -0.2297,  1.0000, -0.3466,  0.7847,  0.7437,  0.8491, -0.2701,  0.2866,\n",
              "          0.9180,  0.3301, -0.6528, -0.8289, -0.7519, -0.4643,  0.6189,  0.3064,\n",
              "          0.5425,  0.8416,  0.7858,  0.4346, -0.0240,  0.1673,  0.9999, -0.1547,\n",
              "         -0.2985, -0.7866, -0.2388, -0.5046, -0.3501,  1.0000,  0.4699,  0.5682,\n",
              "         -0.9942, -0.8524, -0.9738,  1.0000,  0.8859, -0.9468,  0.7765,  0.6741,\n",
              "         -0.2933,  0.8253, -0.4508, -0.3953,  0.2538,  0.1510,  0.9761, -0.6410,\n",
              "         -0.9782, -0.6914,  0.6379, -0.9802,  1.0000, -0.7442, -0.3843, -0.4987,\n",
              "         -0.4596,  0.5722, -0.0291, -0.9888, -0.3789,  0.2586,  0.9856,  0.3686,\n",
              "         -0.7031, -0.9410,  0.8290,  0.7377, -0.8743, -0.9653,  0.9769, -0.9863,\n",
              "          0.5583,  1.0000,  0.3835,  0.0570,  0.3640, -0.5527,  0.4518, -0.5000,\n",
              "          0.7111, -0.9747, -0.4902, -0.2628,  0.5310, -0.2049, -0.3666,  0.7543,\n",
              "          0.2962, -0.6549, -0.7029, -0.3224,  0.5528,  0.8724, -0.3388, -0.1879,\n",
              "          0.1791, -0.1375, -0.9711, -0.5501, -0.5447, -1.0000,  0.8352, -1.0000,\n",
              "          0.5429,  0.1316, -0.3344,  0.8938,  0.6431,  0.6223, -0.8505, -0.7990,\n",
              "          0.6087,  0.8244, -0.5548, -0.5353, -0.7620,  0.4789, -0.2046,  0.4471,\n",
              "         -0.5755,  0.7815, -0.4240,  1.0000,  0.2517, -0.5808, -0.9880,  0.4062,\n",
              "         -0.3166,  1.0000, -0.9380, -0.9721,  0.3990, -0.8491, -0.8926,  0.4127,\n",
              "          0.0737, -0.8650, -0.9518,  0.9744,  0.9468, -0.6671,  0.5932, -0.3868,\n",
              "         -0.6286, -0.0016,  0.8918,  0.9914,  0.3113,  0.9372, -0.3774, -0.0627,\n",
              "          0.9807,  0.2264,  0.7036,  0.2476,  1.0000,  0.4256, -0.9421,  0.1812,\n",
              "         -0.9925, -0.3454, -0.9643,  0.4765,  0.1992,  0.9374, -0.4122,  0.9772,\n",
              "         -0.8657,  0.1333, -0.6193, -0.3864,  0.4077, -0.9710, -0.9881, -0.9870,\n",
              "          0.6853, -0.5384, -0.2409,  0.3393,  0.2190,  0.6077,  0.5857, -1.0000,\n",
              "          0.9585,  0.5813,  0.8969,  0.9779,  0.6469,  0.5948,  0.4246, -0.9894,\n",
              "         -0.9925, -0.4526, -0.3029,  0.7746,  0.8119,  0.8831,  0.4971, -0.5598,\n",
              "         -0.5982, -0.3452, -0.8391, -0.9954,  0.6434, -0.6164, -0.9834,  0.9686,\n",
              "         -0.3286, -0.1404,  0.1743, -0.5664,  0.9791,  0.9060,  0.4962,  0.2004,\n",
              "          0.6272,  0.9482,  0.9678,  0.9892, -0.8332,  0.9156, -0.8277,  0.4746,\n",
              "          0.5964, -0.9537,  0.2273,  0.6838, -0.3867,  0.3942, -0.2683, -0.9839,\n",
              "          0.5946, -0.3581,  0.5818, -0.5789,  0.0021, -0.5070, -0.2122, -0.8122,\n",
              "         -0.7119,  0.7524,  0.6755,  0.9462,  0.8404, -0.2523, -0.8269, -0.2215,\n",
              "         -0.7037, -0.9411,  0.9565, -0.1103, -0.1925,  0.6278, -0.0048,  0.8445,\n",
              "          0.0334, -0.4989, -0.5107, -0.8454,  0.9489, -0.7022, -0.6477, -0.5493,\n",
              "          0.8520,  0.4190,  1.0000, -0.7998, -0.8404, -0.5834, -0.5111,  0.4592,\n",
              "         -0.5506, -1.0000,  0.4514, -0.6357,  0.5531, -0.6082,  0.8223, -0.7593,\n",
              "         -0.9883, -0.3742,  0.4752,  0.7679, -0.5824, -0.7958,  0.7223, -0.1490,\n",
              "          0.9738,  0.9097, -0.6452,  0.0925,  0.7601, -0.3710, -0.6799,  0.9403]],\n",
              "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.last_hidden_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOZWVMw58FAj",
        "outputId": "e029b174-c772-4f69-b37b-048ddd45e13c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.1054,  0.1832, -0.4909,  ..., -0.3241,  0.3984,  0.1591],\n",
              "         [ 0.2438, -0.3654, -0.6240,  ..., -0.0516,  0.3615, -0.3675],\n",
              "         [-0.1027,  0.1559,  0.0640,  ..., -0.5743,  0.0936,  0.2774],\n",
              "         ...,\n",
              "         [ 0.7337,  0.0462, -0.4839,  ..., -0.0072, -0.5861, -0.5907],\n",
              "         [-0.1763,  0.0910, -0.6007,  ...,  0.1243,  0.4635, -0.3595],\n",
              "         [-0.2029, -0.1064, -0.5744,  ...,  0.4472,  0.6531, -0.4184]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.last_hidden_state.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUCCD_nA9Uky",
        "outputId": "6d98e549-77b7-433c-8a52-3779597b3101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 14, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs[0] # same as last hidden state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hmzCY3w9bNg",
        "outputId": "2497b33d-3200-4fe3-9cea-a2a888dcff53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.1054,  0.1832, -0.4909,  ..., -0.3241,  0.3984,  0.1591],\n",
              "         [ 0.2438, -0.3654, -0.6240,  ..., -0.0516,  0.3615, -0.3675],\n",
              "         [-0.1027,  0.1559,  0.0640,  ..., -0.5743,  0.0936,  0.2774],\n",
              "         ...,\n",
              "         [ 0.7337,  0.0462, -0.4839,  ..., -0.0072, -0.5861, -0.5907],\n",
              "         [-0.1763,  0.0910, -0.6007,  ...,  0.1243,  0.4635, -0.3595],\n",
              "         [-0.2029, -0.1064, -0.5744,  ...,  0.4472,  0.6531, -0.4184]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.pooler_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL20h3539iaY",
        "outputId": "49279934-4e6d-4023-e0f9-f33a48ce2d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.9473, -0.4997, -0.8718,  0.8811,  0.7894, -0.3033,  0.9334,  0.4975,\n",
              "         -0.7602, -1.0000, -0.6657,  0.9340,  0.9871,  0.4571,  0.9601, -0.8242,\n",
              "         -0.1882, -0.7192,  0.4297, -0.7576,  0.7524,  1.0000,  0.2104,  0.4064,\n",
              "          0.5661,  0.9898, -0.8310,  0.9563,  0.9695,  0.8257, -0.7848,  0.4256,\n",
              "         -0.9918, -0.3059, -0.8785, -0.9956,  0.5351, -0.8573, -0.0809, -0.1058,\n",
              "         -0.9060,  0.5226,  1.0000, -0.1399,  0.5261, -0.3048, -1.0000,  0.3643,\n",
              "         -0.9460,  0.8846,  0.7668,  0.8117,  0.2881,  0.6517,  0.5824, -0.2660,\n",
              "          0.0118,  0.1780, -0.3221, -0.7566, -0.6814,  0.4362, -0.8407, -0.9596,\n",
              "          0.8762,  0.7554, -0.2953, -0.3134, -0.2160, -0.0667,  0.9666,  0.3424,\n",
              "          0.0743, -0.8780,  0.6402,  0.2758, -0.7399,  1.0000, -0.5168, -0.9872,\n",
              "          0.7122,  0.7422,  0.7109, -0.1680,  0.4112, -1.0000,  0.6379, -0.1462,\n",
              "         -0.9948,  0.2089,  0.5989, -0.3234,  0.3302,  0.7130, -0.4732, -0.5511,\n",
              "         -0.4602, -0.8095, -0.3819, -0.4802,  0.1824, -0.3377, -0.4189, -0.5157,\n",
              "          0.3147, -0.5996, -0.6470,  0.3816, -0.0048,  0.7138,  0.5924, -0.4112,\n",
              "          0.4949, -0.9682,  0.7296, -0.3471, -0.9895, -0.7341, -0.9937,  0.8239,\n",
              "         -0.3895, -0.3774,  0.9832,  0.2486,  0.5444, -0.2425, -0.9053, -1.0000,\n",
              "         -0.6852, -0.1713, -0.0852, -0.2448, -0.9874, -0.9646,  0.6930,  0.9652,\n",
              "          0.3507,  1.0000, -0.5356,  0.9507, -0.3166, -0.7947,  0.5744, -0.5999,\n",
              "          0.7252,  0.4883, -0.7820,  0.3282, -0.5786,  0.1877, -0.7344, -0.3768,\n",
              "         -0.5342, -0.9593, -0.4488,  0.9617, -0.6580, -0.9190,  0.0132, -0.4360,\n",
              "         -0.5127,  0.8982,  0.7884,  0.4786, -0.4304,  0.5496,  0.2895,  0.6779,\n",
              "         -0.8821, -0.1980,  0.5438, -0.4306, -0.7744, -0.9862, -0.5156,  0.6959,\n",
              "          0.9913,  0.8366,  0.3724,  0.8100, -0.4484,  0.7909, -0.9716,  0.9887,\n",
              "         -0.2762,  0.2494, -0.0682,  0.2337, -0.9039, -0.0255,  0.9031, -0.8000,\n",
              "         -0.9244, -0.1093, -0.5341, -0.5321, -0.7741,  0.6869, -0.4124, -0.5095,\n",
              "         -0.2599,  0.9482,  0.9917,  0.8116,  0.1873,  0.7582, -0.9626, -0.5284,\n",
              "          0.2655,  0.4019,  0.2757,  0.9947, -0.6125, -0.3087, -0.9465, -0.9873,\n",
              "          0.0295, -0.9428, -0.2360, -0.7415,  0.7049, -0.5888,  0.5658,  0.4980,\n",
              "         -0.9931, -0.8473,  0.4439, -0.5102,  0.5792, -0.2948,  0.6668,  0.8968,\n",
              "         -0.7408,  0.6627,  0.9338, -0.7714, -0.8636,  0.8998, -0.3689,  0.9307,\n",
              "         -0.7885,  0.9982,  0.8852,  0.8113, -0.9649, -0.6236, -0.9545, -0.6879,\n",
              "         -0.1642,  0.0901,  0.8598,  0.7472,  0.4841,  0.4896, -0.6265,  0.9993,\n",
              "         -0.8253, -0.9635, -0.3037, -0.3706, -0.9913,  0.8040,  0.4028,  0.3308,\n",
              "         -0.5436, -0.7377, -0.9666,  0.9334,  0.3143,  0.9961, -0.4325, -0.9580,\n",
              "         -0.7158, -0.9488, -0.0229, -0.2287, -0.3045, -0.0570, -0.9769,  0.5914,\n",
              "          0.6860,  0.7058, -0.8151,  0.9995,  1.0000,  0.9809,  0.9387,  0.9618,\n",
              "         -0.9999, -0.6581,  1.0000, -0.9914, -1.0000, -0.9708, -0.7347,  0.4207,\n",
              "         -1.0000, -0.1913, -0.0066, -0.9400,  0.6133,  0.9789,  0.9978, -1.0000,\n",
              "          0.9457,  0.9712, -0.7599,  0.9351, -0.4134,  0.9786,  0.4849,  0.6907,\n",
              "         -0.4154,  0.5477, -0.9038, -0.9119, -0.6370, -0.7353,  0.9977,  0.2083,\n",
              "         -0.8779, -0.9488,  0.6546, -0.0561, -0.2484, -0.9743, -0.3691,  0.5904,\n",
              "          0.7463,  0.3033,  0.4226, -0.8184,  0.3554, -0.2778,  0.6312,  0.7593,\n",
              "         -0.9528, -0.7666, -0.1611, -0.1131, -0.5096, -0.9678,  0.9837, -0.6137,\n",
              "          0.8217,  1.0000,  0.1718, -0.9574,  0.7098,  0.3458,  0.0616,  1.0000,\n",
              "          0.8249, -0.9897, -0.6642,  0.7551, -0.7042, -0.7526,  0.9999, -0.3963,\n",
              "         -0.6082, -0.4559,  0.9839, -0.9915,  0.9928, -0.9337, -0.9776,  0.9842,\n",
              "          0.9614, -0.5498, -0.8179,  0.1782, -0.6581,  0.2918, -0.9793,  0.7885,\n",
              "          0.5491, -0.2452,  0.9236, -0.8597, -0.6369,  0.4439, -0.4519,  0.0215,\n",
              "          0.8996,  0.5861, -0.2864,  0.1808, -0.4304, -0.2213, -0.9835,  0.5373,\n",
              "          1.0000, -0.1097,  0.5654, -0.4920, -0.2247,  0.0190,  0.6075,  0.6431,\n",
              "         -0.3465, -0.9370,  0.6292, -0.9790, -0.9924,  0.8198,  0.2410, -0.3098,\n",
              "          1.0000,  0.5190,  0.3379,  0.3158,  0.9834,  0.0082,  0.6649,  0.8096,\n",
              "          0.9853, -0.3637,  0.6894,  0.9305, -0.8707, -0.4580, -0.7281,  0.0286,\n",
              "         -0.9205,  0.0083, -0.9816,  0.9805,  0.9303,  0.4153,  0.3625,  0.5934,\n",
              "          1.0000, -0.6325,  0.7459, -0.5007,  0.8037, -0.9997, -0.9086, -0.4583,\n",
              "         -0.0590, -0.6986, -0.3440,  0.3194, -0.9842,  0.7109,  0.7074, -0.9904,\n",
              "         -0.9906, -0.1722,  0.9065,  0.2480, -0.9800, -0.7749, -0.6696,  0.7887,\n",
              "         -0.3360, -0.9624, -0.0304, -0.4221,  0.6519, -0.4084,  0.6637,  0.7864,\n",
              "          0.7471, -0.6538, -0.4711, -0.2918, -0.8675,  0.8240, -0.9192, -0.9288,\n",
              "         -0.2297,  1.0000, -0.3466,  0.7847,  0.7437,  0.8491, -0.2701,  0.2866,\n",
              "          0.9180,  0.3301, -0.6528, -0.8289, -0.7519, -0.4643,  0.6189,  0.3064,\n",
              "          0.5425,  0.8416,  0.7858,  0.4346, -0.0240,  0.1673,  0.9999, -0.1547,\n",
              "         -0.2985, -0.7866, -0.2388, -0.5046, -0.3501,  1.0000,  0.4699,  0.5682,\n",
              "         -0.9942, -0.8524, -0.9738,  1.0000,  0.8859, -0.9468,  0.7765,  0.6741,\n",
              "         -0.2933,  0.8253, -0.4508, -0.3953,  0.2538,  0.1510,  0.9761, -0.6410,\n",
              "         -0.9782, -0.6914,  0.6379, -0.9802,  1.0000, -0.7442, -0.3843, -0.4987,\n",
              "         -0.4596,  0.5722, -0.0291, -0.9888, -0.3789,  0.2586,  0.9856,  0.3686,\n",
              "         -0.7031, -0.9410,  0.8290,  0.7377, -0.8743, -0.9653,  0.9769, -0.9863,\n",
              "          0.5583,  1.0000,  0.3835,  0.0570,  0.3640, -0.5527,  0.4518, -0.5000,\n",
              "          0.7111, -0.9747, -0.4902, -0.2628,  0.5310, -0.2049, -0.3666,  0.7543,\n",
              "          0.2962, -0.6549, -0.7029, -0.3224,  0.5528,  0.8724, -0.3388, -0.1879,\n",
              "          0.1791, -0.1375, -0.9711, -0.5501, -0.5447, -1.0000,  0.8352, -1.0000,\n",
              "          0.5429,  0.1316, -0.3344,  0.8938,  0.6431,  0.6223, -0.8505, -0.7990,\n",
              "          0.6087,  0.8244, -0.5548, -0.5353, -0.7620,  0.4789, -0.2046,  0.4471,\n",
              "         -0.5755,  0.7815, -0.4240,  1.0000,  0.2517, -0.5808, -0.9880,  0.4062,\n",
              "         -0.3166,  1.0000, -0.9380, -0.9721,  0.3990, -0.8491, -0.8926,  0.4127,\n",
              "          0.0737, -0.8650, -0.9518,  0.9744,  0.9468, -0.6671,  0.5932, -0.3868,\n",
              "         -0.6286, -0.0016,  0.8918,  0.9914,  0.3113,  0.9372, -0.3774, -0.0627,\n",
              "          0.9807,  0.2264,  0.7036,  0.2476,  1.0000,  0.4256, -0.9421,  0.1812,\n",
              "         -0.9925, -0.3454, -0.9643,  0.4765,  0.1992,  0.9374, -0.4122,  0.9772,\n",
              "         -0.8657,  0.1333, -0.6193, -0.3864,  0.4077, -0.9710, -0.9881, -0.9870,\n",
              "          0.6853, -0.5384, -0.2409,  0.3393,  0.2190,  0.6077,  0.5857, -1.0000,\n",
              "          0.9585,  0.5813,  0.8969,  0.9779,  0.6469,  0.5948,  0.4246, -0.9894,\n",
              "         -0.9925, -0.4526, -0.3029,  0.7746,  0.8119,  0.8831,  0.4971, -0.5598,\n",
              "         -0.5982, -0.3452, -0.8391, -0.9954,  0.6434, -0.6164, -0.9834,  0.9686,\n",
              "         -0.3286, -0.1404,  0.1743, -0.5664,  0.9791,  0.9060,  0.4962,  0.2004,\n",
              "          0.6272,  0.9482,  0.9678,  0.9892, -0.8332,  0.9156, -0.8277,  0.4746,\n",
              "          0.5964, -0.9537,  0.2273,  0.6838, -0.3867,  0.3942, -0.2683, -0.9839,\n",
              "          0.5946, -0.3581,  0.5818, -0.5789,  0.0021, -0.5070, -0.2122, -0.8122,\n",
              "         -0.7119,  0.7524,  0.6755,  0.9462,  0.8404, -0.2523, -0.8269, -0.2215,\n",
              "         -0.7037, -0.9411,  0.9565, -0.1103, -0.1925,  0.6278, -0.0048,  0.8445,\n",
              "          0.0334, -0.4989, -0.5107, -0.8454,  0.9489, -0.7022, -0.6477, -0.5493,\n",
              "          0.8520,  0.4190,  1.0000, -0.7998, -0.8404, -0.5834, -0.5111,  0.4592,\n",
              "         -0.5506, -1.0000,  0.4514, -0.6357,  0.5531, -0.6082,  0.8223, -0.7593,\n",
              "         -0.9883, -0.3742,  0.4752,  0.7679, -0.5824, -0.7958,  0.7223, -0.1490,\n",
              "          0.9738,  0.9097, -0.6452,  0.0925,  0.7601, -0.3710, -0.6799,  0.9403]],\n",
              "       grad_fn=<TanhBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.pooler_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6L6Orp2J94l7",
        "outputId": "ec3ff02c-6b9f-49d2-b379-d19c6a8db84d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I0csATFO-zSv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}